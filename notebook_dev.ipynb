{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09991b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.helpers import read_json, create_folders, mysql_qry\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import os\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import regex as re\n",
    "#from distutils.util import strtobool\n",
    "from scipy import stats\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import ast\n",
    "from distutils.util import strtobool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d770b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################\n",
    "# Function that create the required folders and defines paths for the data processing \n",
    "def init_paths(analysis_name : str,\n",
    "               database_name : str) -> list:\n",
    "    \n",
    "    \"\"\"\n",
    "    Function that create the required folders and defines paths for the data processing.\n",
    "    the folders final path will be: `main_folder/analysis_name/database_name`, for example.\n",
    "    the output will be list of 4 paths:\"data_raw/...\", \"data_temp/...\", \"results_figures/...\", \"results_tables/...\"\n",
    "    analysis name : str -> name of the anlysis performed, for example \"lpa_analysis\".\n",
    "    database_name : name of the database, imported from the `config.json` file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Defining and creating the main folders\n",
    "    paths = [\"data_raw\", \"data_temp\", \"results_figures\", \"results_tables\"]\n",
    "    create_folders(paths)\n",
    "\n",
    "    # Defining and creating the sub-folders according to the input arguments.\n",
    "    for i in [analysis_name, database_name]:\n",
    "        paths = [os.path.join(j, i) for j in paths]\n",
    "        create_folders(paths)\n",
    "\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Function that create report file and update it's values based on input\n",
    "def reports(report_name : str,\n",
    "            step : str):\n",
    "    \"\"\"\n",
    "    report_name : str -> name of the report file in the format of `analysis-database_report` (.txt will be added later).\n",
    "    step_name : str / list -> the line which be added to the report, if several lines the input should be a list.\n",
    "    \"\"\"\n",
    "    # Creating reports folder\n",
    "    create_folders([\"reports\"])\n",
    "    \n",
    "    # Initilizing paths and names\n",
    "    report_path = os.path.join(\"reports\", report_name+\".txt\")\n",
    "    report_bool = os.path.exists(report_path)\n",
    "    name_splited = report_name.split(\"-\")\n",
    "    steps_dict = {True:step, False:[step]}\n",
    "    steps = steps_dict[isinstance(step, (list, tuple))]\n",
    "    \n",
    "    lines_count = 1\n",
    "    if report_bool:\n",
    "        with open(report_path, \"r\") as report:\n",
    "            lines_count = len(report.readlines())\n",
    "\n",
    "\n",
    "    # Creating new report file and adding header, if file doesnt exists\n",
    "    # If report file already exists -> add the required line only\n",
    "    with open(report_path, \"a\") as report:\n",
    "        if report_bool is False:\n",
    "            try:\n",
    "                report.write(f\"> Report File of {name_splited[0].replace(\"_\",\" \")} Analysis ({name_splited[1]} database).\\n\")\n",
    "            except:\n",
    "                report.write(f\"> Report File of {report_name}\\n\")\n",
    "\n",
    "        for i in steps:\n",
    "            report.write(f\"{lines_count} {i}\\n\")\n",
    "            lines_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ae7058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> folder `data_raw` exists, continuing.\n",
      "> folder `data_temp` exists, continuing.\n",
      "> folder `results_figures` exists, continuing.\n",
      "> folder `results_tables` exists, continuing.\n",
      "> folder `data_raw\\substitution_sruvival` exists, continuing.\n",
      "> folder `data_temp\\substitution_sruvival` exists, continuing.\n",
      "> folder `results_figures\\substitution_sruvival` exists, continuing.\n",
      "> folder `results_tables\\substitution_sruvival` exists, continuing.\n",
      "> folder `data_raw\\substitution_sruvival\\covid_vaccine_new` exists, continuing.\n",
      "> folder `data_temp\\substitution_sruvival\\covid_vaccine_new` exists, continuing.\n",
      "> folder `results_figures\\substitution_sruvival\\covid_vaccine_new` exists, continuing.\n",
      "> folder `results_tables\\substitution_sruvival\\covid_vaccine_new` exists, continuing.\n",
      "> Established connecntion to the covid_vaccine_new database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing MySQL tables: 100%|██████████| 3/3 [00:00<00:00, 5361.27tables/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> table `sample_metadata.csv` alread exists.\n",
      "> Finished importing raw tabels (['clones.csv', 'clone_stats.csv', 'sample_metadata.csv']).\n",
      "> MySQL connenction terminated.\n",
      "> folder `reports` exists, continuing.\n",
      "> sample_metadata_df.csv already created, continuing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> clones_merged dataframe exists, loading and continuing....\n",
      "> mut_df dataframe exists, loading and continuing....\n",
      "> folder `reports` exists, continuing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_29900\\999036483.py:391: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['subj_3' 'subj_3' 'subj_3' ... 'subj_7' 'subj_7' 'subj_7']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  self.filt_mut_df.loc[:,val_list] = self.filt_mut_df.loc[:,val_list].replace(remap_vals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 2.sp.subj_7 (unique clones:1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'sub_mut' object has no attribute 'os'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 553\u001b[0m\n\u001b[0;32m    550\u001b[0m rename_ab_type \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msn\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-Spike B\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msp\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filt_mut_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mab_target\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()]\n\u001b[0;32m    551\u001b[0m rename_subj \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubj_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m filt_mut_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()]\n\u001b[1;32m--> 553\u001b[0m \u001b[43msubsurv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremap_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrename_time_point\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrename_ab_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrename_subj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 461\u001b[0m, in \u001b[0;36msub_mut.correlate\u001b[1;34m(self, remap_list, aa_range)\u001b[0m\n\u001b[0;32m    458\u001b[0m         df_output\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[i],inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    459\u001b[0m         output_summery \u001b[38;5;241m=\u001b[39m output_summery[output_summery[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m!=\u001b[39m i]\n\u001b[1;32m--> 461\u001b[0m output_summery\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnunique\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mpath[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummery_nclones.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03m* Importing statistics and re modules\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;124;03m* Create empty dataframe with the names of the output_df datasets\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;124;03m* Getting spearman_r values between each of the fequencies dataset and putting them into the spearman output df.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'sub_mut' object has no attribute 'os'"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# Subtitution survival analysis class\n",
    "class sub_mut():\n",
    "    def __init__(self):  \n",
    "        # Getting the config information and database name\n",
    "        self.config_info = read_json(\"config.json\")\n",
    "        self.db_name = self.config_info[\"database\"][\"db_name\"]\n",
    "        self.analysis_type = \"substitution_sruvival\"\n",
    "\n",
    "\n",
    "        # Initilizing the analysis paths and folders\n",
    "        # folder output order: \"data_raw\", \"data_temp\", \"results_figures\", \"results_tables\"\n",
    "        self.paths = init_paths(self.analysis_type, self.db_name)\n",
    "        self.report_name = f\"{self.analysis_type}-{self.db_name}_report\"\n",
    "\n",
    "        # Importing the required raw tables from the MySQL server\n",
    "        req_tables = self.config_info[self.analysis_type][\"req_tables\"].split(\",\")\n",
    "        sql_engine = mysql_qry()\n",
    "        imported_tabeles = []\n",
    "\n",
    "        # Importing each required table from the MySQL server\n",
    "        for i in tqdm(req_tables, desc=\"Importing MySQL tables\", unit=\"tables\"):\n",
    "            i_table_path = os.path.join(self.paths[0], f\"{i}.csv\")\n",
    "            i_exists = os.path.exists(i_table_path)\n",
    "\n",
    "            # Cheecking if the raw table already exsits, if so importing\n",
    "            if i_exists is False:\n",
    "                temp_qry = f\"\"\"\n",
    "                            SELECT * FROM {self.db_name}.{i};\n",
    "                            \"\"\"\n",
    "                \n",
    "                # Incase of invalid input error\n",
    "                try:\n",
    "                    temp_df = sql_engine.run_qry(temp_qry)\n",
    "                    temp_df.to_csv(i_table_path)\n",
    "                    imported_tabeles.append(i)\n",
    "\n",
    "                except:\n",
    "                    print(f\"> Invalid database or table name. (db={self.db_name}, table={i})\")\n",
    "                    continue\n",
    "        \n",
    "        # I table already exists -> moving on\n",
    "        else:\n",
    "            print(f\"> table `{i}.csv` alread exists.\")\n",
    "\n",
    "        # Verifing the existance of all of the rquired tables\n",
    "        \n",
    "        if (np.sort([\"clones\", \"clone_stats\", \"sample_metadata\"]) == np.sort([i.split(\".\")[0] for i in os.listdir(self.paths[0])])).all():\n",
    "            # Finishing report\n",
    "            print(f\"> Finished importing raw tabels ({os.listdir(self.paths[0])}).\")\n",
    "            sql_engine.close_conn()\n",
    "\n",
    "            # Adding to the report -> Successfully imported (or verified) MySQL tables.\n",
    "            reports(self.report_name,\"Successfully imported (or verified) MySQL tables.\")\n",
    "        \n",
    "        else:\n",
    "            raise Exception(\"Verify config file for correct tables values.\")\n",
    "\n",
    "\n",
    "    # Creation of mutation table\n",
    "    def mutation_table(self):\n",
    "        \"\"\"\n",
    "        * Creating orginized metadata dataframe with the information provided by the config.json file.\n",
    "        * Saving the metadata_df into folder.\n",
    "        * If the dataframe already exists, load it without processing.\n",
    "        \"\"\"\n",
    "        path_processed_dir = self.paths[1]\n",
    "        path_metdadata_df = os.path.join(path_processed_dir, \"sample_metadata_df.csv\")\n",
    "\n",
    "        if exists(path_metdadata_df):\n",
    "            print(\"> sample_metadata_df.csv already created, continuing...\")\n",
    "            metadata_df = pd.read_csv(path_metdadata_df, index_col=0)\n",
    "                                                                                        \n",
    "        else:\n",
    "            print(\"> Creating sample_metadata_df.csv.\")\n",
    "        \n",
    "            metadata_keys_og = self.config_info[\"substitution_sruvival\"][\"req_metadata\"].split(\",\")\n",
    "            metadata_keys_new =  self.config_info[\"substitution_sruvival\"][\"new_metadata_labels\"].split(\",\")\n",
    "            meta_dict = dict(zip(metadata_keys_og, metadata_keys_new))\n",
    "\n",
    "            metadata_df = pd.read_csv(os.path.join(self.paths[0], \"sample_metadata.csv\"), index_col=0)\n",
    "            metadata_og = metadata_df[metadata_df[\"key\"].isin(metadata_keys_og)]\n",
    "            metadata_og = metadata_og.replace({\"key\":meta_dict})\n",
    "\n",
    "            sample_ids = np.sort(metadata_og[\"sample_id\"].unique())\n",
    "            metadata_df = pd.DataFrame({\"sample_id\":sample_ids})\n",
    "            metadata_df[metadata_keys_new] = np.nan\n",
    "\n",
    "            for i in sample_ids:\n",
    "                temp_sid = i\n",
    "                for j in metadata_keys_new:\n",
    "                    cond_sid = (metadata_og[\"sample_id\"] == i)\n",
    "                    cond_key = (metadata_og[\"key\"] == j)\n",
    "                    metadata_df.loc[metadata_df[\"sample_id\"]==i,j] = metadata_og.loc[(metadata_og[\"sample_id\"]==i)&(metadata_og[\"key\"]==j),\"value\"].values\n",
    "            metadata_df.to_csv(path_metdadata_df)\n",
    "            print(\"> Done.\")\n",
    "\n",
    "        # Creation of filtred metadata table\n",
    "        \"\"\"\n",
    "        * Creating custom function to pull metadata from metadata_df\n",
    "        * sample_id validation\n",
    "        \"\"\"\n",
    "        def assign_metadata(sample_id, meta_df):\n",
    "            meta_list = meta_df.columns[1:]\n",
    "            meta_sample = meta_df.loc[meta_df[\"sample_id\"]==sample_id, meta_list].values.flatten()\n",
    "            return meta_sample\n",
    "        \n",
    "        clone_stats = pd.read_csv(os.path.join(self.paths[0], \"clone_stats.csv\"), index_col=0)\n",
    "        metalist_sids = np.sort(metadata_df.sample_id.unique())\n",
    "        clones_sids = np.sort(clone_stats.dropna().sample_id.unique()).astype(\"int\")\n",
    "\n",
    "        values_missing = np.setdiff1d(clones_sids, metalist_sids)\n",
    "        values_common = np.intersect1d(metalist_sids, clones_sids)\n",
    "\n",
    "        if len(values_missing) > 0:\n",
    "            print(\"> missing sample_id from metadata file:\",values_missing)\n",
    "            clone_stats = clone_stats[clone_stats[\"sample_id\"].isin(values_common)]\n",
    "            raise TypeError(\"verify metadata sample_id values\") \n",
    "        \n",
    "        # Merging clones and clones status > adding the relevent metadata to the dataframe\n",
    "        \"\"\"\n",
    "        * loading clones_merged if exists, if not creating and saving\n",
    "        * custom function that extract mutations infromation from the \"mutation\" json in each row\n",
    "        * Adding the germline infromation to the clone_stats df\n",
    "        * Dropping null sample_id rows (cannot assign metadata for those rows)\n",
    "        * converting \"sample_id\" values to int instead of floats\n",
    "        * assiging the metadata into the merged table (applying assign_metadata)\n",
    "        * renaming id_x to id after merging (left had \"id\" colum while right had \"id\"==\"clone_id\")\n",
    "        * reseting the index\n",
    "        \"\"\"\n",
    "        \n",
    "        path_clones_merged = os.path.join(self.paths[1], \"clones_merged.csv\")\n",
    "\n",
    "        if exists(path_clones_merged):\n",
    "            clones_merged = pd.read_csv(path_clones_merged)\n",
    "            print(\"> clones_merged dataframe exists, loading and continuing....\")\n",
    "\n",
    "        else: \n",
    "            clones = pd.read_csv(os.path.join(self.paths[0], \"clones.csv\"))\n",
    "            clones_merged = clone_stats.merge(right=clones[[\"id\",\"germline\"]],\n",
    "                                                how=\"left\",\n",
    "                                                left_on=\"clone_id\",\n",
    "                                                right_on=\"id\")    \n",
    "            \n",
    "            clones_merged = clones_merged[clones_merged[\"sample_id\"].notnull()]        \n",
    "            clones_merged[list(metadata_df.columns)[1:]] = list(clones_merged[\"sample_id\"].apply(assign_metadata, args=(metadata_df,)))\n",
    "            clones_merged.rename({\"id_x\":\"id\"},axis=\"columns\",inplace=True)\n",
    "            clones_merged.reset_index(drop=True, inplace=True)\n",
    "            clones_merged.to_csv(path_clones_merged)\n",
    "            print(\"> clones_merged dataframe created and saved, continuing....\")\n",
    "\n",
    "        # Creating the mutation dataframe\n",
    "        \"\"\"\n",
    "        * Creating df with the relevent mutations infromation for each clone (mut_df)\n",
    "        * Cleaning the mut_df and adding relevent data\n",
    "        * Saving the mut_df\n",
    "        \"\"\"\n",
    "\n",
    "        path_mut_df = os.path.join(self.paths[-1], \"mut_df.csv\")\n",
    "\n",
    "        if exists(path_mut_df):\n",
    "            mut_df = pd.read_csv(path_mut_df,index_col=0)\n",
    "            print(\"> mut_df dataframe exists, loading and continuing....\")\n",
    "\n",
    "        else: \n",
    "            def mut_regall(string):\n",
    "                pattern = r\"'pos': (?P<position>\\d+), 'from_nt': '(?P<from_nt>[\\w]+)', 'from_aa': '(?P<from_aa>[\\w\\*]+)', 'to_nt': '(?P<to_nt>['\\w\\*]+)', 'to_aas': \\[(?P<to_aas>['\\w,\\s\\*]+)], 'unique': (?P<unique>\\d+), 'total': (?P<total>\\d+), 'intermediate_aa': '(?P<intermediate_aa>[\\w\\d\\*])'\"\n",
    "            \n",
    "                tjson = json.loads(string)\n",
    "                \n",
    "                if \"ALL\" in str(tjson[\"regions\"].keys()):\n",
    "                    all_value = str(tjson[\"regions\"][\"ALL\"])\n",
    "                    find = re.findall(pattern,all_value)\n",
    "                    return find\n",
    "                \n",
    "                else:\n",
    "                    else_value = str(tjson[\"regions\"])\n",
    "                    return else_value\n",
    "                    \n",
    "            clones_merged[\"regions_all\"] = clones_merged[\"mutations\"].apply(mut_regall)\n",
    "            clones_raval = clones_merged.copy()\n",
    "            ra_val = []\n",
    "            \n",
    "            for i in range(0,len(clones_raval)):\n",
    "                length = len(clones_raval.loc[i,\"regions_all\"]) # length of the list, number of mutations is the colum\n",
    "                value = clones_raval.loc[i,\"regions_all\"] # the value mutations themselves list of lists/ list / np.nan\n",
    "                id_value = clones_raval.loc[i,\"id\"] # id value of the row\n",
    "                clone_id = clones_raval.loc[i,\"clone_id\"] # clone_id value of the row\n",
    "                subject_id = clones_raval.loc[i,\"subject_id\"]# subject_id value of the row\n",
    "                sample_id = clones_raval.loc[i,\"sample_id\"] # sample_id value of the row\n",
    "                funct = clones_raval.loc[i,\"functional\"] # functional value of the clone\n",
    "                total_cnt = clones_raval.loc[i,\"total_cnt\"] # target of the antibody\n",
    "                unique_cnt = clones_raval.loc[i,\"unique_cnt\"] # unique sequence is clone\n",
    "                germline = clones_raval.loc[i,\"germline\"] #germline sequence\n",
    "                top_seq = clones_raval.loc[i,\"top_copy_seq_sequence\"] #top copy of sequence\n",
    "                metadata = clones_raval.loc[i,metadata_df.columns[1:]].values.flatten().tolist() #metadata list value\n",
    "                ins_val = [id_value, clone_id, subject_id, sample_id, funct, total_cnt,unique_cnt, germline, top_seq] + metadata\n",
    "                \n",
    "                # if single row of mutation\n",
    "                if length == 1:\n",
    "                    to_aas = value[0][4].replace(\" \",\"\").replace(\"''\",\"\").split(\",\")\n",
    "                    \n",
    "                    if (len(to_aas) == 1):\n",
    "                        temp_list = list(value[0])\n",
    "                        ra_val.append(ins_val + temp_list) \n",
    "                        \n",
    "                    else:\n",
    "                        for i in range(0,len(to_aas)):\n",
    "                            temp_list = list(value[0])\n",
    "                            temp_list[4] = to_aas[i]\n",
    "                            ra_val.append(ins_val + temp_list)\n",
    "                \n",
    "                # if multiple rows of mutations\n",
    "                if length > 1:\n",
    "                    for j in range(0,length):\n",
    "                            sub_value = list(value[j]) #each row\n",
    "                            temp_list = sub_value\n",
    "                            \n",
    "                            # making sure that the length of the list is correct, in some rows there is missing values\n",
    "                            if len(sub_value) == 8:\n",
    "                                to_aas = sub_value[4].replace(\" \",\"\").replace(\"'\",\"\").split(\",\")\n",
    "                                \n",
    "                                if len(to_aas) == 1:\n",
    "                                    ra_val.append(ins_val + temp_list)\n",
    "                                elif len(to_aas) > 1:\n",
    "                                    for aa in set(to_aas): # set() removes duplicate values\n",
    "                                        temp_list[4] = aa\n",
    "                                        ra_val.append(ins_val + temp_list)\n",
    "                                                \n",
    "                elif length == 0:\n",
    "                    ra_val.append(ins_val + np.full(shape=len(value), fill_value=np.nan).tolist())\n",
    "            \n",
    "            mut_df_cols = [\"id\", \"clone_id\", \"subject_id\", \"sample_id\", \"functional\", \"total_cnt\",\"unique_cnt\", \"germline\", \"top_seq\"]\n",
    "            mut_info_cols = [\"pos\",\"from_nt\",\"from_aa\",\"to_nt\",\"to_aas\",\"unique\",\"total\",\"intermidiate_aa\"]\n",
    "            \n",
    "            mut_df = pd.DataFrame(data=ra_val, columns = mut_df_cols + metadata_df.columns[1:].tolist() + mut_info_cols)\n",
    "            mut_df[\"to_aas\"] = mut_df[\"to_aas\"].str.replace(\"'\",\"\") #cleaning to_aas string\n",
    "            mut_df.replace({\"to_aas\":{\"None\":np.nan}}, inplace=True) #turining none values to np.nan\n",
    "            mut_df.dropna(axis=0,subset=[\"pos\",\"to_aas\"], ignore_index=True, inplace=True) #dropping null rows of \"pos\" and \"to_aas\"\n",
    "\n",
    "            # custom function to round numbers upward\n",
    "            def round_up(number):\n",
    "                num_dec = number\n",
    "                num_round = round(number)\n",
    "                \n",
    "                if num_round < num_dec:\n",
    "                    value = num_round + 1\n",
    "                else:\n",
    "                    value = num_round\n",
    "                return value\n",
    "            \n",
    "            mut_df.insert(6,\"pos_aa\",np.nan) #inserting amino acid position column\n",
    "            mut_df.insert(6,\"pos_nt\",np.nan) #inserting nucleotide position column\n",
    "            mut_df.loc[:,\"pos_nt\"] = mut_df.loc[:,\"pos\"].apply(int)+1 #filling the pos_nt column\n",
    "            mut_df.loc[:,\"pos_aa\"] = ((mut_df.loc[:,\"pos_nt\"])/3).apply(round_up) #fillint the pos_aa column \n",
    "            mut_df.astype({\"pos_nt\":\"int\",\"pos_aa\":\"int\"})\n",
    "            mut_df.drop(axis=1,columns=\"pos\",inplace=True) #dropping the og column (it was -1 in position...)\n",
    "            mut_df[\"syn\"] = (mut_df[\"from_aa\"] == mut_df[\"to_aas\"]).apply(int) #creating syn column\n",
    "\n",
    "            mut_df.to_csv(path_mut_df)\n",
    "            print(\"> mut_df dataframe created and saved, continuing....\")\n",
    "            \n",
    "        reports(self.report_name, \"Successfully created mutation dataframe (mut_df.csv)\")\n",
    "\n",
    "\n",
    "    def filter_mutdf(self,\n",
    "                        aa_range : tuple = (1,104),\n",
    "                        unique_seq_filt : int = 1,\n",
    "                        save_filtdf : bool = False\n",
    "                        ):\n",
    "        \n",
    "        ###\n",
    "        \"\"\"\n",
    "        aa_range : tuple / list -> range of the amino acids position which will be included in the analysis.\n",
    "        unique_seq_filt : int -> Threshold (>) of required unique sequences per clone.\n",
    "        save_filtdf : bool -> To save the filtred dataframe into the temp_data folder.\n",
    "        * only functional clones\n",
    "        * Include only non-synonymous mutations. \n",
    "        \"\"\"\n",
    "        try:\n",
    "            mut_df = pd.read_csv(os.path.join(self.paths[-1], \"mut_df.csv\"))\n",
    "\n",
    "        except:\n",
    "            raise Exception(f\"> No `mut_df.csv` table found in {self.paths[-1]} folder, please run the sub_mut.mutation_table() method.\")\n",
    "\n",
    "        filt_pos_aa = (mut_df[\"pos_aa\"].between(aa_range[0], aa_range[1], inclusive='both')) # from aa positions 1->104\n",
    "        filt_unique_cnt = (mut_df[\"unique_cnt\"] > unique_seq_filt) # only clones with more than 1 unique sequence\n",
    "        filt_functional = (mut_df[\"functional\"] == 1) # only functional clones\n",
    "        filt_syn = (mut_df[\"syn\"] == 0) # only non-syn mutations (substitutions)\n",
    "\n",
    "        self.filt_mut_df = mut_df[filt_pos_aa & filt_functional &  filt_syn & filt_unique_cnt]\n",
    "\n",
    "        if save_filtdf:\n",
    "            self.filt_mut_df.to_csv(os.path.join(self.paths[1], \"filt_mut_df.csv\"))\n",
    "\n",
    "        return self.filt_mut_df\n",
    "\n",
    "    # Creation of correlation dataframe and correlationsmap\n",
    "    def correlate(self,\n",
    "                    remap_list : list = None,\n",
    "                    aa_range : tuple = (1,104)):\n",
    "        \"\"\"\n",
    "        remap_list = if we want to remap the metadata columns values, we can input list for dicts with values to change (same order as metdata).\n",
    "        aa_range : tuple / list -> range of the amino acids position which will be included in the analysis.\n",
    "        \"\"\"\n",
    "\n",
    "        ###\n",
    "        \"\"\"\n",
    "        * Custom function that creates matrix of all possible conditions across the metadata\n",
    "        \"\"\"\n",
    "        def metadata_cond(dic_input):\n",
    "            mt_dic = dic_input\n",
    "            mt_keys = mt_dic.keys()\n",
    "            mt_keys_len = np.array([len(mt_dic[i]) for i in mt_keys])\n",
    "\n",
    "            n_columns = len(mt_keys)\n",
    "            n_rows = np.prod(mt_keys_len)\n",
    "            meta_df = pd.DataFrame(np.zeros((n_rows,n_columns)), columns = mt_keys)\n",
    "\n",
    "            for i,v,l in zip(range(0,len(mt_keys)), mt_keys, mt_keys_len):\n",
    "                array_length = n_rows\n",
    "                unique_val = v\n",
    "                \n",
    "                if i == 0:          \n",
    "                    reps_numbers = np.prod(mt_keys_len[i+1:])\n",
    "\n",
    "                    temp_array = []\n",
    "                    for val_i in mt_dic[unique_val]:\n",
    "                        temp_array+=[val_i for i in range(1,reps_numbers+1)]\n",
    "\n",
    "                    meta_df[v] = temp_array\n",
    "\n",
    "                else:\n",
    "                    reps_numbers = np.prod(mt_keys_len[i+1:])\n",
    "                    temp_array = []\n",
    "                    \n",
    "                    for val_i in mt_dic[unique_val]:\n",
    "                        temp_array += [val_i for k in range(1,reps_numbers+1)]\n",
    "\n",
    "                    final_array = temp_array.copy()\n",
    "                    for num in range(1,int(array_length/len(temp_array))):\n",
    "                        final_array += temp_array.copy()\n",
    "\n",
    "                    meta_df[v] = final_array   \n",
    "                    \n",
    "            return meta_df\n",
    "        \n",
    "        ###\n",
    "        \"\"\"\n",
    "        * Creating the metadata dic\n",
    "        * Using the custom function metadata_cond to create the conditions matrix\n",
    "        \"\"\"\n",
    "        catagories_dict = {}\n",
    "        change_order = bool(strtobool(self.config_info[\"substitution_sruvival\"][\"fig_order_change\"]))\n",
    "        if change_order:\n",
    "            val_list = self.config_info[\"substitution_sruvival\"][\"fig_order\"].split(\",\")\n",
    "            \n",
    "            for i in val_list:\n",
    "                temp_val_list = list(self.filt_mut_df[i].unique())\n",
    "                temp_val_list.sort()\n",
    "                catagories_dict[i] = temp_val_list\n",
    "\n",
    "\n",
    "        else:\n",
    "            unique_subj = list(self.filt_mut_df[\"subject_id\"].unique())\n",
    "            \n",
    "            catagories_dict[\"subject_id\"] = unique_subj\n",
    "            val_list = list(metadata_df.columns[1:])\n",
    "            val_list.insert(0,\"subject_id\")\n",
    "            \n",
    "            for i in val_list:\n",
    "                catagories_dict[i] = np.sort(self.filt_mut_df[i].unique())   \n",
    "        \n",
    "        ###\n",
    "        \"\"\"\n",
    "        * option to remap the metadata values if remap == True\n",
    "        * creation of remaping dictionary (if remap == True)\n",
    "        * Creating metadata matrix\n",
    "        \"\"\"\n",
    "        remap = bool(strtobool(self.config_info[\"substitution_sruvival\"][\"fig_remap\"]))\n",
    "        if remap:\n",
    "            remap_name = self.db_name  + \";\" + \".\".join(val_list)\n",
    "            remap_path = \"remapping\\\\\"+remap_name+\"-remap\"+\".txt\"\n",
    "            catag_path = \"remapping\\\\\"+remap_name+\"-catag\"+\".txt\"\n",
    "\n",
    "\n",
    "            custom_list = remap_list\n",
    "            remap_vals = {old_val:new_val for old_dic,new_dic in zip(catagories_dict.values(),custom_list) for old_val,new_val in zip(old_dic,new_dic)}\n",
    "            catagories_dict = {i:j for i,j in zip(list(catagories_dict.keys()),custom_list)}\n",
    "                    \n",
    "            self.filt_mut_df.loc[:,val_list] = self.filt_mut_df.loc[:,val_list].replace(remap_vals)\n",
    "            catagories_dict = {i:j for i,j in zip(list(catagories_dict.keys()),[i for i in catagories_dict.values()])}\n",
    "            \n",
    "            sep = \".\"\n",
    "        \n",
    "        else:\n",
    "            sep = \"|\"\n",
    "\n",
    "        cond_matrix = metadata_cond(catagories_dict).astype(\"str\")\n",
    "        cond_matrix.to_csv(os.path.join(self.paths[1], \"metadata_matrix.csv\"))\n",
    "        \n",
    "        ###\n",
    "        \"\"\"\n",
    "        * Defining custom function to get frequences of surviving non-syn clones.\n",
    "        \"\"\"\n",
    "        def get_output(df, cname, prange, method):\n",
    "            df_output = pd.DataFrame({\"pos_aa\":pos_range})\n",
    "            total_unique = len(df[\"clone_id\"].unique())\n",
    "            temp_num = df.groupby(\"pos_aa\").agg({\"clone_id\":\"nunique\"}).reset_index()\n",
    "            \n",
    "            if method == \"freq\":\n",
    "                temp_freq = pd.DataFrame({\"pos_aa\":temp_num[\"pos_aa\"],cname:temp_num[\"clone_id\"]/total_unique})\n",
    "            elif method == \"count\":\n",
    "                temp_freq = pd.DataFrame({\"pos_aa\":temp_num[\"pos_aa\"],cname:temp_num[\"clone_id\"]}) \n",
    "        \n",
    "            df_output = df_output.merge(temp_freq, on=\"pos_aa\", how=\"left\")\n",
    "            df_output.fillna(0,inplace=True)               \n",
    "            return df_output\n",
    "        \n",
    "        ###\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # https://www.imgt.org/IMGTScientificChart/Numbering/IMGTIGVLsuperfamily.html\n",
    "        range_all = range(aa_range[0],aa_range[1]+1) #FR1, CDR1, FR2, CDR2, FR3\n",
    "\n",
    "        df_input = self.filt_mut_df.copy()\n",
    "        pos_range = range_all\n",
    "        output_summerylis = []\n",
    "        df_output = pd.DataFrame({\"pos_aa\":pos_range})\n",
    "\n",
    "        for i in range(0,len(cond_matrix)):\n",
    "            df_temp = df_input.copy()\n",
    "            temp_vals = cond_matrix.iloc[i,:].values\n",
    "\n",
    "            for name, val in zip(val_list,temp_vals): \n",
    "                df_temp = df_temp[df_temp[name] == val]\n",
    "\n",
    "            col_name = sep.join(temp_vals)\n",
    "            output_summerylis.append([col_name,len(df_temp[\"clone_id\"].unique())])\n",
    "            \n",
    "            df_getoutput = get_output(df_temp,col_name,pos_range,method=\"freq\")\n",
    "            df_output = df_output.merge(df_getoutput, on=\"pos_aa\", how=\"left\")\n",
    "\n",
    "        df_output.to_csv(os.path.join(self.paths[-1], \"substitution_fractions.csv\"))\n",
    "\n",
    "        ###\n",
    "        \"\"\"\n",
    "        * Creates dataframe with the filtred dataset values and their unique clones.\n",
    "        * Giving report on small datasets.\n",
    "        \"\"\"\n",
    "        # creates dataframe with the filtred dataset values and their unique clones.\n",
    "        output_summery = pd.DataFrame(output_summerylis, columns=[\"dataset\",\"nunique\"])\n",
    "        small_datasets = list(output_summery.loc[output_summery[\"nunique\"]<int(self.config_info[\"substitution_sruvival\"][\"clone_filter\"]),\"dataset\"].values)\n",
    "\n",
    "        if len(small_datasets) > 0:\n",
    "            for i in small_datasets:\n",
    "                print(\"Dropped \"+i+\" (unique clones:\"+ str(output_summery.loc[output_summery[\"dataset\"]==i,\"nunique\"].values[0])+\")\")\n",
    "                df_output.drop(columns=[i],inplace=True)\n",
    "                output_summery = output_summery[output_summery[\"dataset\"]!= i]\n",
    "\n",
    "        output_summery.sort_values(\"nunique\").to_csv(os.path.join(self.os.path[-1], \"summery_nclones.csv\"))\n",
    "\n",
    "        ###\n",
    "        \"\"\"\n",
    "        * Importing statistics and re modules\n",
    "        * Create empty dataframe with the names of the output_df datasets\n",
    "        * Getting spearman_r values between each of the fequencies dataset and putting them into the spearman output df.\n",
    "        \"\"\"\n",
    "\n",
    "        names = list(df_output.columns)[1:] #order by name of the filtered loop\n",
    "        spearmanr_df = pd.DataFrame(data=np.nan,index=names,columns=names)\n",
    "        spearman_list = []\n",
    "        \n",
    "        for i in names:\n",
    "            for j in names:\n",
    "                re_pattern = r\"statistic=np\\.float64\\(([\\d\\.\\-e]+)\\), pvalue=np.float64\\(([\\d\\.\\-e]+)\\)\\)\" #updated\n",
    "                temp_string = str(stats.spearmanr(df_output[i],df_output[j]))\n",
    "                temp_spearmanr = re.findall(re_pattern,temp_string)[0][0]\n",
    "                temp_pvalue = re.findall(re_pattern,temp_string)[0][1]\n",
    "\n",
    "                spearmanr_df.loc[i,j] = float(temp_spearmanr)\n",
    "                spearman_list.append([i+ \" X \" +j, temp_spearmanr, temp_pvalue])\n",
    "\n",
    "                # if some value isnt statisticly sifnificant print the value.\n",
    "                if float(temp_pvalue) > 0.05:\n",
    "                    print(i+ \" x \" +j +\" Not Significant\")\n",
    "\n",
    "        # spearman_r matrix output\n",
    "        spearman_summery = pd.DataFrame(spearman_list,columns=[\"dataset\", \"spearman_r\", \"p_value\"]) \n",
    "\n",
    "        ###\n",
    "        \"\"\"\n",
    "        * importing visualision module\n",
    "        * creating heatmap graph\n",
    "        \"\"\"\n",
    "\n",
    "        if len(spearmanr_df)/4 < 5:\n",
    "            size_graph = 5\n",
    "        else:\n",
    "            size_graph = len(spearmanr_df)/4\n",
    "\n",
    "        fig, ax = plt.subplots(1,1, figsize=(size_graph ,size_graph ))\n",
    "\n",
    "        ax = sns.heatmap(spearmanr_df,\n",
    "                        cbar_kws={'label': 'Spearman r'},\n",
    "                        xticklabels = True, \n",
    "                        yticklabels = True,\n",
    "                        #cmap=sns.diverging_palette(250, 20, s=150, l=50, n=20),\n",
    "                        cmap = sns.color_palette(\"coolwarm\", as_cmap=True, n_colors=1),\n",
    "                        vmin = round(spearmanr_df.min().min(),1)-0.05, \n",
    "                        vmax = 1)\n",
    "        ax.invert_yaxis()\n",
    "        ax.axis('scaled')\n",
    "\n",
    "        labels_xy = labels=spearmanr_df.columns\n",
    "        #ax.set_yticklabels(labels_xy, rotation=45, ha='right')\n",
    "        #ax.set_xticklabels(labels_xy, rotation=45, ha='right')\n",
    "\n",
    "        hide_ticklabels = False\n",
    "        if hide_ticklabels:\n",
    "            ax.tick_params(\n",
    "                axis='both',       # changes apply to both x and y axes\n",
    "                which='both',       # both major and minor ticks are affected\n",
    "                bottom=False,       # ticks along the bottom edge are off\n",
    "                top=False,          # ticks along the top edge are off\n",
    "                left=False,         # ticks along the left edge are off\n",
    "                right=False,        # ticks along the right edge are off\n",
    "                labelbottom=False,  # labels along the bottom edge are off\n",
    "                labelleft=False)    # labels along the left edge are off\n",
    "            \n",
    "\n",
    "        #save_path = \"figures_output\\\\{}\\\\\".format(config[\"mysql\"][\"db\"])\n",
    "        #photo_name = \"{}\".format(config[\"mysql\"][\"db\"])+\"-\"+\".\".join(val_list)\n",
    "                                \n",
    "        #if os.path.exists(save_path) == False:\n",
    "        #    os.mkdir(save_path)\n",
    "\n",
    "        #time_output = datetime.now().strftime('[%d-%m-%Y %H;%M] ')\n",
    "        #plt.savefig(save_path+time_output+photo_name+\".png\",bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "subsurv = sub_mut()\n",
    "subsurv.mutation_table()\n",
    "subsurv.filter_mutdf()\n",
    "\n",
    "filt_mut_df = subsurv.filt_mut_df\n",
    "rename_time_point = [i[0] for i in filt_mut_df[\"time_point\"].unique()]\n",
    "rename_ab_type = [\"sn\" if i == 'Non-Spike B' else \"sp\" for i in filt_mut_df[\"ab_target\"].unique()]\n",
    "rename_subj = [\"subj_\"+str(i) for i in filt_mut_df[\"subject_id\"].unique()]\n",
    "\n",
    "subsurv.correlate(remap_list = [rename_time_point, rename_ab_type, rename_subj])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
